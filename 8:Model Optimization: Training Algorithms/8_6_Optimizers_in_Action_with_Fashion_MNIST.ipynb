{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYN2fQFJINwzKiUn7wVWlu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soyebganja/DL-Practice-Projects/blob/main/8%3AModel%20Optimization%3A%20Training%20Algorithms/8_6_Optimizers_in_Action_with_Fashion_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "45avz2PAVI0t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyuvHFKbV06g",
        "outputId": "b707e752-51bd-469c-a8bc-d3d91fe5c1cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 15.5MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 308kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.48MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 12.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "0oKKeSbSWgh5",
        "outputId": "b13cf00b-27d5-4442-f3d0-56928dd30cfb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchvision.datasets.mnist.FashionMNIST"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torchvision.datasets.mnist.FashionMNIST</b><br/>def __init__(root: Union[str, Path], train: bool=True, transform: Optional[Callable]=None, target_transform: Optional[Callable]=None, download: bool=False) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/torchvision/datasets/mnist.py</a>`Fashion-MNIST &lt;https://github.com/zalandoresearch/fashion-mnist&gt;`_ Dataset.\n",
              "\n",
              "Args:\n",
              "    root (str or ``pathlib.Path``): Root directory of dataset where ``FashionMNIST/raw/train-images-idx3-ubyte``\n",
              "        and  ``FashionMNIST/raw/t10k-images-idx3-ubyte`` exist.\n",
              "    train (bool, optional): If True, creates dataset from ``train-images-idx3-ubyte``,\n",
              "        otherwise from ``t10k-images-idx3-ubyte``.\n",
              "    download (bool, optional): If True, downloads the dataset from the internet and\n",
              "        puts it in root directory. If dataset is already downloaded, it is not\n",
              "        downloaded again.\n",
              "    transform (callable, optional): A function/transform that  takes in a PIL image\n",
              "        and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
              "    target_transform (callable, optional): A function/transform that takes in the\n",
              "        target and transforms it.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 204);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "G4JU6QWFWrJF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_dataloader:\n",
        "    print(images.shape)\n",
        "    print(labels.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahB4yhBxXUhM",
        "outputId": "0f39f2ed-2595-4378-c7f9-cc6ab482af07"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(images[7].squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "PMyYGaW4XgbO",
        "outputId": "72e43804-79ca-4f4c-a08e-7a1e8e13ebd0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMkAAADICAYAAABCmsWgAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAEthJREFUeJzt3W9M1dUfB/A3GFxA4CKSF0kw2txss+FEIGZ/rJjkmonyoLIHlpXLwA1plbjUzbVRupVmVFsr7c+U5gNw6bIVGq6GNFHXzMZskeDwXmPGHxEB4fweNO9+9/f9HM/9cu/l3uvv/druAz4evpzvvX36cg6fc06MUkqBiLRiw90BokjHJCEyYJIQGTBJiAyYJEQGTBIiAyYJkQGThMiASUJkwCQhMrgjVBeuq6vDjh074Ha7kZeXh927d6OwsND4fePj4+ju7kZKSgpiYmJC1T36P6eUwsDAALKyshAba3hWqBCor69X8fHx6rPPPlO//fabeumll1RaWpryeDzG7+3q6lIA+OJrUl5dXV3G/yZjlAp+gWNRUREKCgrwwQcfAPj36ZCdnY3169dj48aNt/zevr4+pKWlBbtLEW3RokVivKOjwxLr7u4O+Ofl5ORYYgsWLBDbNjY2BvzzIllvby+cTuct2wT9162RkRG0tbWhpqbGG4uNjUVJSQlaWlos7YeHhzE8POz9emBgINhduiU7v9KF4P8nAIA77pA/BuOvARMkXTcuLi4kPyvS+fP5B/1T6OnpwdjYGFwul0/c5XLB7XZb2tfW1sLpdHpf2dnZwe4SUUDCPrtVU1ODvr4+76urqyvcXSLyEfRftzIyMjBlyhR4PB6fuMfjQWZmpqW9w+GAw+EIdjf8pnvcjo+P+32NWbNmWWJr1qwR27766quWWGpqqt8/K1TGxsbE+JdffmmJvfHGG2LbXbt2BdQH3a+Xdj6LUAj6kyQ+Ph75+floamryxsbHx9HU1ITi4uJg/ziikAvJ30mqq6uxevVqLFy4EIWFhdi5cycGBwfx/PPPh+LHEYVUSJLkqaeewt9//40tW7bA7XZj/vz5OHLkiGUwTxQNQvYX98rKSlRWVobq8kSTJuyzW0SRLiR/cQ9Ef3+/8S+gEyXNntiZOTl16pQYnzNnjiWWkJAgtr127ZolNjg4KLaVrvHPP/+IbXt7ey2xmTNnim2TkpL86hcAJCYmWmLJycli2ytXrlhiP/zwg9j22WefFeOSQD+3W+nr6zPOLvJJQmTAJCEyYJIQGTBJiAxCNgUcTsEoNZEqlu+77z6xrVS4qSu1keZJ4uPjxbZSqYhU2gMAWVlZlphuMD4yMmKJSQN0ABgaGvIrBsiVxKtWrRLbTp061RIrKysT20qfm+4zDsU8FJ8kRAZMEiIDJgmRAZOEyIBJQmRwW85u2ZnhWLFihRgvKiqyxC5evCi2lWZadGvGpZkaXX+luG4PAKkPukVMUlvdoitp1ks3S3jjxg1LrLOzU2y7ZMkSS2zp0qVi22+//dYSm8xqKj5JiAyYJEQGTBIiAyYJkUHUryeZMmWKJaYbhEp0t9/T02OJ6TaRk9ZySGUXumvoBsLSANvOID8YAu2DNJjXXVdXciOti5FKgQD5/dX1AeB6EqKgYJIQGTBJiAyYJEQGTBIig6gvS7Ezk3Xw4EFLTJqZAoCrV69aYrNnzxbbStewU7qhE6qjF+yQZqx0s1vSZyHNPgLyDjG6xVyLFy+2xOrr6/3uQ6DC/ykQRTgmCZEBk4TIgElCZBD1A3c77JyPIu1gotuhI9AyGDslJZN9bLed/tpZpyKtt9FtDbtw4UJLTDdw524pRGHAJCEyYJIQGTBJiAyYJEQGUb/oyo4LFy5YYrqf1d/fb4llZ2eLbf/8809LTLe/rzSrMzo6KraVFhDZKQnRlbVIcTvX1ZHuTVdqIr0/usVPUomQ7oAiu7joiigImCREBkwSIgMmCZHBbVmWkpeXJ8YzMjIsMWmADsglEtLhN7q2169fF9vaOUlWiuvaSgNvO9e1QzchIE1A6Mpopk2bZonp3l87a3BCgU8SIgMmCZEBk4TIgElCZGA7SY4fP45ly5YhKysLMTExaGxs9Pl3pRS2bNmCmTNnIjExESUlJTh//nyw+ks06WzPbg0ODiIvLw9r1qzBypUrLf++fft2vP/++/j888+Rm5uLzZs3o7S0FOfOndMuqgk23Z690s4dunIMaS9fXYmGnUN8Aj1sR9dWmrEKxiE+Et11pVko3W4pUltdH2bNmuV330LBdpIsXbpUeyKRUgo7d+7Em2++ieXLlwMAvvjiC7hcLjQ2NuLpp58OrLdEYRDUMUlHRwfcbjdKSkq8MafTiaKiIrS0tIjfMzw8jP7+fp8XUSQJapLc3A7f5XL5xF0ul3ar/NraWjidTu9LV2lLFC5hn92qqalBX1+f99XV1RXuLhH5CGpZys1DWDwej0+9v8fjwfz588XvcTgccDgcwewGFixYIMalwbRu4C4NTnVlE9KaieTkZLGt7hoSO6UmEl1b3WDa37Z2vl83yJdO9dWdLCytJ5FORwaA1tZWv/vmr6A+SXJzc5GZmYmmpiZvrL+/H62trba28yGKJLafJFevXsUff/zh/bqjowNnzpxBeno6cnJyUFVVhbfeegtz5szxTgFnZWWhrKwsmP0mmjS2k+TkyZN45JFHvF9XV1cDAFavXo29e/fi9ddfx+DgINauXYve3l488MADOHLkyKT9jYQo2GwnyeLFi2+5S15MTAy2bduGbdu2BdQxokgR9tktokh3Wy660i30sbPgSbeDSaB9kMoxdLN7UpmGruTGTlmKHdJvDbr+9vX1WWK6o7qlGTLd4irp51VVVYltn3nmGTEeCD5JiAyYJEQGTBIiAyYJkcFtOXDXlTdIdINbqXxEN5i3s+7D3++3e41Qke5ZN3kQ6CBfd7/Dw8OW2GT+3S38nwJRhGOSEBkwSYgMmCREBkwSIoPbcnZr06ZNYlyaqbFTCpGeni627enpscQm+yjpQOkWUkmzfLpSHuk90+0aI81ASguxAHlRm27phfS+B3pOFZ8kRAZMEiIDJgmRAZOEyOC2HLjfc889Ylwqb9CVTUhx6fReQB5w6gbuEXbYsZF0H7odX6QdYuyUsOgmD6Rr/PXXX35fN1B8khAZMEmIDJgkRAZMEiIDJgmRQdTPbt11112WWFJSkthWKh/RtbVTjiHNyujaBnpEtW62yM4CLWkXFt0BOtJ1pVlC4N9jNv6XbqGadIR3amqq2FYqHZrM0wf4JCEyYJIQGTBJiAyYJEQGUT9wf/DBB/1uKw1O4+PjxbbSwF0abALyOhPdgNXOwTx2Siwms9xFV5Zy7do1S0x3bykpKZaYblJCet/tHCQUKD5JiAyYJEQGTBIiAyYJkQGThMgg6me37By2I5VT6Mo5pMVGaWlpYlvpGrp+2SlLkdra2bvYTqmKndki3eyWNAulayvNCOr6oNvRZrLwSUJkwCQhMmCSEBkwSYgMon7g3tzc7HdbOyUhUgmLbiAsDSx1ay6kwamupEQq09ANbqX+6nZskdrqrmtnQC+9P7pSEymuG6CHe4cZPkmIDJgkRAZMEiIDJgmRga0kqa2tRUFBAVJSUjBjxgyUlZWhvb3dp83169dRUVGB6dOnIzk5GeXl5fB4PEHtNNFksjW71dzcjIqKChQUFODGjRvYtGkTlixZgnPnzmHq1KkAgA0bNuDw4cM4cOAAnE4nKisrsXLlSvz8888huYEnnnjC77ZSiYSubOLOO++0xHTJbqckRJrV0c2wSbM9uhkrO4fXSH3Q9Ve6hu5gHjuLo+zMbul2cpkstpLkyJEjPl/v3bsXM2bMQFtbGx566CH09fXh008/xb59+/Doo48CAPbs2YN7770XJ06cwP333x+8nhNNkoDGJDcPrL9ZrNbW1obR0VGUlJR428ydOxc5OTloaWkRrzE8PIz+/n6fF1EkmXCSjI+Po6qqCosWLcK8efMAAG63G/Hx8ZZqWZfLBbfbLV6ntrYWTqfT+5rMTceI/DHhJKmoqMDZs2dRX18fUAdqamrQ19fnfXV1dQV0PaJgm1BZSmVlJQ4dOoTjx49j1qxZ3nhmZiZGRkbQ29vr8zTxeDzIzMwUr+VwOLQH6fjj8ccf97uttMZDVz4i7eaxbt06se1XX31liel2YZFOndUN3KUJAd0g1k65i53yHOmzSUhIENtK25zqyoZmz55tifX29opt7XC5XJZYoLOrtp4kSilUVlaioaEBR48eRW5urs+/5+fnIy4uDk1NTd5Ye3s7Ojs7UVxcHFBHicLF1pOkoqIC+/btw8GDB5GSkuIdZzidTiQmJsLpdOKFF15AdXU10tPTkZqaivXr16O4uJgzWxS1bCXJRx99BABYvHixT3zPnj147rnnAADvvfceYmNjUV5ejuHhYZSWluLDDz8MSmeJwsFWkvhTspyQkIC6ujrU1dVNuFNEkYS1W0QGUb/oSppFkmaQAHhLZ/6bblZH0tDQIMZ3795tia1atUpsK82aTZ8+XWzb3d1tidmZCbSzx7CuPCcjI8MS082wtba2WmK7du0S2z788MOWmJ3DjHSefPJJS+yTTz7x+/slfJIQGTBJiAyYJEQGTBIig6gfuEuDUGlwDASn7EGyceNGv2J2SeUfunuzs57EzsB9MquydWtlpLUnQ0NDYttly5ZZYhy4E4UYk4TIgElCZMAkITJgkhAZRP3s1osvvmiJlZeXi22TkpIsMd0uIeHeoQOQdx/RHZMdbTo6OiwxaYcaQJ6V1C38CsWuPHySEBkwSYgMmCREBkwSIoOoH7hLgzppJw5AHtRJO3wAwP79+wPqlx26yQM7p+/aOejGTltpLYdufYed0pjvvvvOEpMmYQC5FOfw4cNi23feeUeMB4JPEiIDJgmRAZOEyIBJQmTAJCEyiPrZLUlnZ6cYl3Ya0S1i+u89jk2kXVgGBwf9/v5g7BISCaT9iHUH85w5c8YSk/ZqBoDk5GRLbDL3deOThMiASUJkwCQhMmCSEBnclgN33a4br732miV25coVse2lS5f8/nm6g4D+39gpd7l8+bIlptsBRdrJZTInNfgkITJgkhAZMEmIDJgkRAYRN3C3M/izew1pAKj7K6+djSCC0efbgZ33QXrfdVuqSm11f8m3y58+x6gI+4QvXryI7OzscHeD/k90dXUZS5AiLknGx8fR3d2NlJQUDAwMIDs7G11dXUhNTQ1314Kqv7+f9xZGSikMDAwgKytLu9rzpoj7dSs2Ntab2Tf/3pGamhqxb3ageG/ho1u6/b84cCcyYJIQGUR0kjgcDmzdutXWibPRgvcWPSJu4E4UaSL6SUIUCZgkRAZMEiIDJgmRQUQnSV1dHe6++24kJCSgqKgIv/zyS7i7ZNvx48exbNkyZGVlISYmBo2NjT7/rpTCli1bMHPmTCQmJqKkpATnz58PT2dtqK2tRUFBAVJSUjBjxgyUlZWhvb3dp83169dRUVGB6dOnIzk5GeXl5fB4PGHq8cRFbJJ8/fXXqK6uxtatW3Hq1Cnk5eWhtLRUXNEWyQYHB5GXl6fdAmf79u14//338fHHH6O1tRVTp05FaWlpxJ9o1dzcjIqKCpw4cQLff/89RkdHsWTJEp+tlDZs2IBvvvkGBw4cQHNzM7q7u7Fy5cow9nqCVIQqLCxUFRUV3q/HxsZUVlaWqq2tDWOvAgNANTQ0eL8eHx9XmZmZaseOHd5Yb2+vcjgcav/+/WHo4cRdvnxZAVDNzc1KqX/vIy4uTh04cMDb5vfff1cAVEtLS7i6OSER+SQZGRlBW1sbSkpKvLHY2FiUlJSgpaUljD0Lro6ODrjdbp/7dDqdKCoqirr77OvrAwCkp6cDANra2jA6Oupzb3PnzkVOTk7U3VtEJklPTw/Gxsbgcrl84i6XC263O0y9Cr6b9xLt9zk+Po6qqiosWrQI8+bNA/DvvcXHxyMtLc2nbbTdGxCBVcAUfSoqKnD27Fn89NNP4e5KSETkkyQjIwNTpkyxzIR4PB5kZmaGqVfBd/Neovk+KysrcejQIRw7dsxn8VJmZiZGRkYsJ5FF073dFJFJEh8fj/z8fDQ1NXlj4+PjaGpqQnFxcRh7Fly5ubnIzMz0uc/+/n60trZG/H0qpVBZWYmGhgYcPXoUubm5Pv+en5+PuLg4n3trb29HZ2dnxN+bRbhnDnTq6+uVw+FQe/fuVefOnVNr165VaWlpyu12h7trtgwMDKjTp0+r06dPKwDq3XffVadPn1YXLlxQSin19ttvq7S0NHXw4EH166+/quXLl6vc3Fw1NDQU5p7f2rp165TT6VQ//vijunTpkvd17do1b5uXX35Z5eTkqKNHj6qTJ0+q4uJiVVxcHMZeT0zEJolSSu3evVvl5OSo+Ph4VVhYqE6cOBHuLtl27NgxBcDyWr16tVLq32ngzZs3K5fLpRwOh3rsscdUe3t7eDvtB+meAKg9e/Z42wwNDalXXnlFTZs2TSUlJakVK1aoS5cuha/TE8RSeSKDiByTEEUSJgmRAZOEyIBJQmTAJCEyYJIQGTBJiAyYJEQGTBIiAyYJkQGThMiASUJk8B8HJmDfl6fU4gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels[7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu7JWdCjXmE0",
        "outputId": "54b9126b-297d-4f89-c032-2684e1b81247"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\"cuda\" if torch.cuda.is_available()\n",
        "else \"mps\" if torch.backends.mps.is_available()\n",
        "else \"cpu\")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtOyM9UeXu8G",
        "outputId": "e7bf2d6a-7eca-44a6-fea8-638f0fbc4c1c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ClothsClasifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.network = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(28*28, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 10)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.network(x)\n"
      ],
      "metadata": {
        "id": "qlPuipiEaVt6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ClothsClasifier().to(device)\n",
        "# optimiser = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "WJ293apscfuk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 2\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for batch, (images, labels) in enumerate(train_dataloader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # 1. Forward pass\n",
        "    optimiser.zero_grad()\n",
        "    predictions = model(images)\n",
        "    loss = loss_fn(predictions, labels)\n",
        "\n",
        "    # 2. Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # 3. Update wieght\n",
        "    optimiser.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print(f\"Epoch: {epoch} | Batch: {batch} | Loss: {loss.item()}\")\n",
        "\n",
        "# SGD without momemtum (2.31 ==> 2.06)\n",
        "# SGD with momumtum(2.30 ==> 0.65)\n",
        "# Adam (2.30 ==> 0.48)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLaaXvMsc6YX",
        "outputId": "8e7698c6-e4ad-424a-fb41-d8d3772020ff"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Batch: 0 | Loss: 2.302257776260376\n",
            "Epoch: 0 | Batch: 100 | Loss: 0.8133978247642517\n",
            "Epoch: 0 | Batch: 200 | Loss: 0.45040082931518555\n",
            "Epoch: 0 | Batch: 300 | Loss: 0.6168220639228821\n",
            "Epoch: 0 | Batch: 400 | Loss: 0.5167648196220398\n",
            "Epoch: 0 | Batch: 500 | Loss: 0.4622653126716614\n",
            "Epoch: 0 | Batch: 600 | Loss: 0.42125260829925537\n",
            "Epoch: 0 | Batch: 700 | Loss: 0.6220594644546509\n",
            "Epoch: 0 | Batch: 800 | Loss: 0.5496124625205994\n",
            "Epoch: 0 | Batch: 900 | Loss: 0.5034422278404236\n",
            "Epoch: 1 | Batch: 0 | Loss: 0.3198843002319336\n",
            "Epoch: 1 | Batch: 100 | Loss: 0.41299158334732056\n",
            "Epoch: 1 | Batch: 200 | Loss: 0.3227578103542328\n",
            "Epoch: 1 | Batch: 300 | Loss: 0.4545401930809021\n",
            "Epoch: 1 | Batch: 400 | Loss: 0.422526478767395\n",
            "Epoch: 1 | Batch: 500 | Loss: 0.3721804916858673\n",
            "Epoch: 1 | Batch: 600 | Loss: 0.35186341404914856\n",
            "Epoch: 1 | Batch: 700 | Loss: 0.5758815407752991\n",
            "Epoch: 1 | Batch: 800 | Loss: 0.4910585582256317\n",
            "Epoch: 1 | Batch: 900 | Loss: 0.480493426322937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "all_predicted = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        all_predicted.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())"
      ],
      "metadata": {
        "id": "nVArXcN1fNYE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwub4kDXi2Kz",
        "outputId": "4242e64c-8148-48ca-de2e-d14bd1d0d805"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[np.int64(9), np.int64(2), np.int64(1), np.int64(1), np.int64(6)]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_predicted[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWz4y_Xki_h4",
        "outputId": "dac8f642-9dd9-47d4-de85-bb031540db8d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[np.int64(9), np.int64(2), np.int64(1), np.int64(1), np.int64(6)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = classification_report(all_labels, all_predicted)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ndErtmzjMSl",
        "outputId": "f709177b-4229-42d2-adfd-a01c00fe5bc6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.79      0.81      1000\n",
            "           1       0.97      0.97      0.97      1000\n",
            "           2       0.83      0.60      0.70      1000\n",
            "           3       0.82      0.88      0.85      1000\n",
            "           4       0.65      0.88      0.75      1000\n",
            "           5       0.97      0.92      0.94      1000\n",
            "           6       0.70      0.60      0.65      1000\n",
            "           7       0.89      0.96      0.92      1000\n",
            "           8       0.93      0.98      0.95      1000\n",
            "           9       0.96      0.93      0.95      1000\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.86      0.85      0.85     10000\n",
            "weighted avg       0.86      0.85      0.85     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yVVHW6Xgjmkf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}